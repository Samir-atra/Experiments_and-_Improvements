{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9dXK9kMI645S",
        "outputId": "12562db6-e13d-460c-f928-2f2fdbdf1132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython\n",
        "import sys\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import PIL.Image\n",
        "from PIL import ImageOps\n",
        "import PIL\n",
        "import datetime\n",
        "import tensorboard\n",
        "import sklearn\n",
        "import subprocess\n",
        "import pandas\n",
        "import time\n",
        "import glob\n",
        "from IPython import display\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive             \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qh-wsUzj645Y",
        "outputId": "30a05752-82c7-414b-eb1a-28988b35774e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8144 files belonging to 1 classes.\n",
            "Using 6516 files for training.\n",
            "Found 8144 files belonging to 1 classes.\n",
            "Using 1628 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# Dataset loading\n",
        "\n",
        "data_path = pathlib.Path('/content/drive/MyDrive/CarsDataset')\n",
        "\n",
        "# data_path_test = pathlib.Path('/content/drive/MyDrive/CarsDataset')\n",
        "\n",
        "\n",
        "dataset_path = tf.keras.utils.image_dataset_from_directory(        # Training dataset\n",
        "    data_path,\n",
        "    labels= 'inferred',\n",
        "    label_mode= None,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed= 1,\n",
        "    batch_size=5,\n",
        "    image_size=(28, 28),\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=True)\n",
        "\n",
        "dataset_path_val = tf.keras.utils.image_dataset_from_directory(      #Validation dataset\n",
        "    data_path,\n",
        "    labels= 'inferred',\n",
        "    label_mode= None,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed= 2,\n",
        "    batch_size=5,\n",
        "    image_size=(28, 28),\n",
        "    color_mode=\"grayscale\",\n",
        "    shuffle=True)\n",
        "\n",
        "\n",
        "# (train_images, train_labels) = dataset_path\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "s19yoaUB645Z"
      },
      "outputs": [],
      "source": [
        "# The generator\n",
        "\n",
        "# train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "# train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "    # tf.keras.layers.Reshape((5, 28, 28, 1)),\n",
        "    tf.keras.layers.Rescaling(scale=1 / 127.5, offset=-1),    \n",
        "    tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "    tf.keras.layers.Reshape((7, 7, 256)),\n",
        "    # assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
        "\n",
        "    tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
        "    # assert model.output_shape == (None, 7, 7, 128)\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "    tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "    # assert model.output_shape == (None, 14, 14, 64)\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "    tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'),\n",
        "    # assert model.output_shape == (None, 28, 28, 1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xNgQrjy_645b",
        "outputId": "cd7a3d47-7271-4d62-a875-9f67343623be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2b96274ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Image generation\n",
        "\n",
        "generator = make_generator_model()    \n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ow9-ag3D645b"
      },
      "outputs": [],
      "source": [
        "# The discriminator\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',input_shape=[28, 28, 1]),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
        "    tf.keras.layers.LeakyReLU(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1),\n",
        "])\n",
        "    return model\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yvLNAyqr645c",
        "outputId": "d88d2dd0-d69c-4935-bc97-6cadc9f175d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-0.00096341]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Decision making\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NL4eEkVt645d"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MdxX5A6T645d"
      },
      "outputs": [],
      "source": [
        "# Discriminator loss\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "sfElrMIT645e"
      },
      "outputs": [],
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Snk_SuJ8645f"
      },
      "outputs": [],
      "source": [
        "# Optimizers\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "metadata": {
        "id": "7uHpez_rm3Ql"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HYaLGvGk645g"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dFwRy0nL645g"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "#      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      print(\"fifteenth is here\")\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "      print(\"beedoo\")\n",
        "    # Generate after the final epoch\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epochs,\n",
        "                             seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JdDswPBx645h"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "  plt.savefig('image_at_epoch_{}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(generated_image, cmap=\"gray_r\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "MJ6G-Go4lwxe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "O0b3E49k645i",
        "outputId": "973af7ae-5da9-4039-e6f0-d820d25832ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADEUlEQVR4nO3dwWrCUBBA0U7J///y67Kb8FIaxWt7ztIQCehlwCFx1lofQM/nqy8AOCdOiBInRIkTosQJUcfFcT/lwvPN2YsmJ0SJE6LECVHihChxQpQ4IUqcELXdc86crl9+zB0vf8vV98Hn/VgmJ0SJE6LECVHihChxQpQ4IUqcEDUXuymLK3g+93PCOxEnRIkTosQJUeKEKHFClFvGIMrkhChxQpQ4IUqcECVOiBInRIkTorZ7TntKeB2TE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBC1fTTmlau/CPRoTfg9kxOixAlR4oQocUKUOCFKnBAlToi6tee0x4TnMTkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRx+7gzGxPXms99GKAbyYnRIkTosQJUeKEKHFClDghSpwQtd1z2mPC65icECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROijjsnz8z2+FrrztvDv2ZyQpQ4IUqcECVOiBInRIkTosQJUWMXCU0mJ0SJE6LECVHihChxQpQ4IeoLLnQnzLvU6jIAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Train the model\n",
        "train(dataset_path, EPOCHS)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}