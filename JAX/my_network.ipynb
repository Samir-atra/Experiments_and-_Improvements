{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0/EqlW1SvaTJVQxDH9gR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samir-atra/Machine-Learning/blob/main/JAX/my_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive             \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install jax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmWDYP68bgnl",
        "outputId": "f2eb9cd6-bf0c-42a5-c837-16a4c037dd16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.9/dist-packages (0.4.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from jax) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# build it with guidance from  Dense noise leaf detector.py from MulticlassLR directory in the same repo.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import sys\n",
        "from Dataset_setupWithJAX import dataset_setup\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, jit, grad\n",
        "# from numba import jit\n",
        "\n",
        "X, Y = dataset_setup(\"/content/drive/MyDrive/CitrusDataset/Leaves/Leafdataset/Training/\")\n",
        "X_test, Y_test = dataset_setup(\"/content/drive/MyDrive/CitrusDataset/Leaves/Leafdataset/Testing/\")\n",
        "\n",
        "train_set_flatten = jnp.reshape(X,(X.shape[0],-1)).T\n",
        "test_set_flatten = jnp.reshape(X_test, (X_test.shape[0],-1)).T\n",
        "\n",
        "train_class = jnp.reshape(Y, (1, Y.shape[0]))\n",
        "test_class = jnp.reshape(Y_test, (1, Y_test.shape[0]))\n",
        "\n",
        "train_set_flatten = train_set_flatten / 255.\n",
        "test_set_flatten = test_set_flatten / 255.\n",
        "\n",
        "# n_x = 12288     # num_px * num_px * 3\n",
        "# n_h = 7\n",
        "# n_y = 1\n",
        "# layers_dims = (n_x, n_h, n_y)\n",
        "# learning_rate = 0.0075\n",
        "\n",
        "\n",
        "\n",
        "# n_in = 12288\n",
        "\n",
        "# n_hid1 = 128\n",
        "# n_hid2 = 128\n",
        "# n_hid3 = 128\n",
        "# n_hid4 = 128\n",
        "# n_out = 1\n",
        "\n",
        "layers_dims = (12288, 128, 128, 128, 128, 1)\n",
        "\n",
        "\n",
        "def init_params(layer_dims):\n",
        "\n",
        "    key = jax.random.PRNGKey(2)\n",
        "    params = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "\n",
        "    for l in range(1, L):\n",
        "        params[\"W\"+str(l)] = jax.random.normal(key, shape = (layers_dims[l], layer_dims[l-1])) / jnp.sqrt(layer_dims[l-1]) # * 0.01 \n",
        "        params[\"b\"+str(l)] = jnp.zeros((layer_dims[l], 1))\n",
        "\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "\n",
        "    s = 1 /(1+jnp.exp(-z))\n",
        "    cache = z\n",
        "\n",
        "    return s, cache\n",
        "\n",
        "\n",
        "def sigmoid_back(dA, cache):\n",
        "\n",
        "    Z = cache\n",
        "    s = 1/(jnp.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    # dZ = grad(Z)\n",
        "    # print(\"this is cache from sigmoid_back: \", cache)\n",
        "    # print(\"this is the z derivative from sigmoid back\", dZ)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "\n",
        "    a = jnp.maximum(0, z)\n",
        "    cache = z\n",
        "\n",
        "    return a, cache\n",
        "\n",
        "\n",
        "def relu_back(dA, cache):\n",
        "\n",
        "    Z = cache\n",
        "    dZ = jnp.array(dA, copy=True)\n",
        "\n",
        "    dZ.at[Z <= 0].set(0)\n",
        "\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def forward_prop(A, W, b):\n",
        "\n",
        "    dot = jnp.dot(W, A)\n",
        "    Z = dot + b\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache\n",
        "\n",
        "\n",
        "def forward_activation(A_prev, W, b, activation):\n",
        "    \n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = forward_prop(A_prev, W, b)\n",
        "        A, active_cache = sigmoid(Z)\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = forward_prop(A_prev, W, b)\n",
        "        A, active_cache = relu(Z)\n",
        "    \n",
        "    cache = (linear_cache, active_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "\n",
        "\n",
        "def model_forward(X, params):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(params) // 2\n",
        "\n",
        "    for l in range (1, L):\n",
        "        A_prev = A\n",
        "        A, cache = forward_activation(A_prev, params[\"W\" + str(l)], params[\"b\" + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    AL, cache = forward_activation(A, params[\"W\" + str(L)], params[\"b\" + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "def cost_func(AL, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # l = np.where(AL < 0.0000000001, np.log((AL), np.float64), 0.999999999999)\n",
        "    l = jnp.log((AL))\n",
        "    lo = jnp.log((1-AL))\n",
        "    cost = -(1/m) * (jnp.sum((Y*l)+((1-Y)*(lo))))\n",
        "\n",
        "    cost = jnp.squeeze(cost)\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n",
        "def reg_cost(AL, Y, parameters, lambd):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "\n",
        "    cross_entropy_cost = cost_func(AL, Y)\n",
        "\n",
        "    L2_cost =  (1/m)*(lambd/2)*(jnp.sum(jnp.square(W1)) + jnp.sum(jnp.square(W2)) + jnp.sum(jnp.square(W3)))\n",
        "\n",
        "    cost = cross_entropy_cost + L2_cost\n",
        "    \n",
        "    return cost\n",
        "\n",
        "\n",
        "def back_prop(dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m) * jnp.dot((dZ), (A_prev.T))\n",
        "    db = (1/m) * jnp.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = jnp.dot(W.T, dZ)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def back_active(dA, cache, activation):\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_back(dA, activation_cache)\n",
        "        dA_prev, dW, db = back_prop(dZ, linear_cache)\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_back(dA, activation_cache)\n",
        "        dA_prev, dW, db = back_prop(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "\n",
        "def model_back(AL, Y, caches):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[1]\n",
        "    Y = jnp.reshape(Y,(AL.shape))\n",
        "\n",
        "    dAL = - (jnp.divide(Y, AL) - jnp.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    current_cache = caches[L - 1]\n",
        "    dA_prev_temp, dW_temp, db_temp = back_active(dAL, current_cache, activation = \"sigmoid\")\n",
        "    grads[\"dA\" + str(L - 1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "    grads[\"db\" + str(L)] = db_temp\n",
        "\n",
        "    for l in reversed(range(L - 1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = back_active(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "\n",
        "def update_params(params, grads, learning_rate):\n",
        "\n",
        "    parameters = params.copy()\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for l in range(L):        \n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "\n",
        "def predict(X, y, parameters):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2\n",
        "    p = jnp.zeros((1,m))\n",
        "\n",
        "    probas, caches = model_forward(X, parameters)\n",
        "\n",
        "    for i in range(0, probas.shape[1]):      #     dZ.at[Z <= 0].set(0)\n",
        "        if probas[0,i] > 0.5:  \n",
        "            p.at[0,i].set(1)\n",
        "        else:\n",
        "            p.at[0,i].set(0)\n",
        "\n",
        "    print(\"Accuracy: \"  + str(jnp.sum((p == y)/m)))\n",
        "        \n",
        "    return p\n",
        "\n",
        "\n",
        "def nn_model(X, Y, layers_dims, learning_rate = 0.0075, n_iter = 10000, print_cost = False, lambd = 0.7):\n",
        "    \n",
        "    costs = []\n",
        "\n",
        "    parameters = init_params(layers_dims)\n",
        "\n",
        "    for i in range(0, n_iter):\n",
        "        AL, caches = model_forward(X, parameters)\n",
        "        cost = reg_cost(AL, Y, parameters, lambd)\n",
        "        # cost = cost_func(AL, Y)\n",
        "        grads = model_back(AL, Y, caches)\n",
        "        parameters = update_params(parameters, grads, learning_rate)\n",
        "\n",
        "        if print_cost and i % 100 == 0 or i == n_iter - 1:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, jnp.squeeze(cost)))\n",
        "        if i % 100 == 0 or i == n_iter:\n",
        "            costs.append(cost)\n",
        "    \n",
        "    return parameters, costs\n",
        "\n",
        " \n",
        "parameters, costs = nn_model(train_set_flatten, train_class, layers_dims, learning_rate = 0.0007, n_iter = 1000, print_cost = True, lambd = 0.0005) \n",
        "\n",
        "pred_train = predict(train_set_flatten, train_class, parameters)\n",
        "\n",
        "# print(pred_train.shape)\n",
        "# cm = confusion_matrix(train_class.argmax(axis=1), pred_train.argmax(axis=1))\n",
        "# cm_display = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
        "\n",
        "# cm_display.plot()\n",
        "# plt.show()\n",
        "\n",
        "pred_test = predict(test_set_flatten, test_class, parameters)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYoeYgzgbKaD",
        "outputId": "9eb643dd-c8c1-49e5-ec13-f626e8b4043b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.6787093877792358\n",
            "Cost after iteration 100: 0.5823764801025391\n",
            "Cost after iteration 200: 0.47971779108047485\n",
            "Cost after iteration 300: 0.42452341318130493\n",
            "Cost after iteration 400: 0.39831021428108215\n",
            "Cost after iteration 500: 0.38463857769966125\n",
            "Cost after iteration 600: 0.3766973316669464\n",
            "Cost after iteration 700: 0.3716510832309723\n",
            "Cost after iteration 800: 0.36823174357414246\n",
            "Cost after iteration 900: 0.3657584488391876\n",
            "Cost after iteration 999: 0.3639145493507385\n",
            "Accuracy: 0.49455982\n",
            "Accuracy: 0.48387098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Possibilities\n",
        "to fit the test set well on the network you may use a bigger dev set.\n",
        "\n",
        "things to test optimizing:\n",
        "- learning rate.\n",
        "- batch size.\n",
        "- no. hidden cells.\n",
        "- no. hidden layers.\n",
        "- adam optimizer (beta1, beta2, epsilon).\n",
        "\n",
        "##Record\n",
        "- without regularization the network best result: 0.34 error and 0.76 accuracy. each result with a different learning rate.\n",
        "\n",
        "- with the reg. a run with 0.00005, 2000 iter and 0.7 lambd will result error 0.639 and accuracy 0.526.\n",
        "\n",
        "- with reg a run with 0.0005, 3000 iter and 0.7 lambd. will result training error of 0.4902 and accuracy of 0.7527.\n",
        "\n",
        "- with reg a run with 0.001, 3000 iter and 0.7 lambd, will result training error of 0.4864 and accuracy of 0.75667.\n",
        "\n",
        "- with reg a run with 0.005, 3000 iter and 0.7 lambd, will result training error 0.48048, accuracy 0.57665 and test accuracy 0.55161.\n",
        "\n",
        "- with reg a run with 0.0007, 3000 iter and 0.7 lambd, will result training error 0.48800, accuracy 0.75272 and test accuracy 0.64516.\n",
        "\n",
        "- with reg a run with 0.0007, 4000 iter and 0.001 lambd, will result 0.3537550592501022 as training error, training Accuracy: 0.7537091988130565\n",
        "and test Accuracy: 0.6451612903225807.\n",
        "\n",
        "- with reg a run with 0.0007, 4000 iter and 0.0001 lambd, will result training error 0.35358383575002045 training Accuracy: 0.7537091988130565\n",
        "test Accuracy: 0.6451612903225807.\n",
        "\n",
        "- all of the above was using the architecture: \n",
        "(12288, 128, 128, 128, 128, 1) and now it will be changed.\n",
        "\n",
        "- with reg a run with 0.0007, 4000 iter and 0.001 lambd, arch \n",
        "(12288, 1024, 512, 256, 128, 1) and it result training error of 0.3578575759145978, training accuracy of 0.7378832838773492, and test accuracy of 0.5774193548387099\n",
        "\n",
        "- duo to the time that has been taken by the bigger model will try to use jax to run the model after including adam optimizer and dropout"
      ],
      "metadata": {
        "id": "4DyQ8zQtroIu"
      }
    }
  ]
}