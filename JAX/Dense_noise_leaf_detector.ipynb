{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samir-atra/Private-ML/blob/main/MulticlassLR/Dense_noise_leaf_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be used as a template for a numpy network"
      ],
      "metadata": {
        "id": "HvyfLTjPVpLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vs7uXqeNM21X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30d32c9-7c18-444c-80c5-7002c701fb47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL.Image\n",
        "from PIL import ImageOps\n",
        "import PIL\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import IPython\n",
        "import sklearn\n",
        "import cv2\n",
        "import sys\n",
        "from google.colab import drive             \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = pathlib.Path('/content/drive/MyDrive/LeafDataset/Leaves/Leafdataset/Training/')\n",
        "data_path_test = pathlib.Path('/content/drive/MyDrive/LeafDataset/Leaves/Leafdataset/Testing/')\n",
        "\n",
        "dataset_path = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    labels= 'inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed= 1,\n",
        "    batch_size=5,\n",
        "    image_size=(180, 180),\n",
        "    color_mode=\"rgb\",\n",
        "    shuffle=True)\n",
        "\n",
        "dataset_path_val = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    labels= 'inferred',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed= 2,\n",
        "    batch_size=5,\n",
        "    image_size=(180, 180),\n",
        "    color_mode=\"rgb\",\n",
        "    shuffle=True)\n",
        "\n",
        "dataset_path_test = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path_test,\n",
        "    labels= 'inferred',\n",
        "    seed= 3,\n",
        "    batch_size=5,\n",
        "    image_size=(180, 180),\n",
        "    color_mode=\"rgb\",\n",
        "    shuffle=True)"
      ],
      "metadata": {
        "id": "kwG-6LV8NTJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d666909-fbc7-4ed6-f455-337cb2da55be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 936 files belonging to 2 classes.\n",
            "Using 749 files for training.\n",
            "Found 936 files belonging to 2 classes.\n",
            "Using 187 files for validation.\n",
            "Found 310 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE=tf.data.AUTOTUNE\n",
        "dataset_path = dataset_path.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "dataset_path_val = dataset_path_val.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "abVIAX9tNXKH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Resizing(60, 60),\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.2),                                                                         #DP1\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.2),                                                                         #DP2\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.2),                                                                         #DP3\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.2),                                                                         #DP4\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.2),                                                                         #DP5\n",
        "    tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits= True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    dataset_path,\n",
        "    epochs=10,                                    \n",
        "    validation_data = dataset_path_val)\n",
        "\n",
        "model.evaluate(dataset_path_test, batch_size=5, verbose=2)\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "MIzQzpOlNbeF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd714d1-26cb-4b5e-b0c7-a550a97b7c4b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150/150 [==============================] - 13s 66ms/step - loss: 1.2259 - accuracy: 0.7196 - val_loss: 0.7448 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.7307 - accuracy: 0.9760 - val_loss: 0.6424 - val_accuracy: 0.9840\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.6320 - accuracy: 0.9893 - val_loss: 0.5775 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.5807 - accuracy: 0.9947 - val_loss: 0.5483 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 3s 17ms/step - loss: 0.5723 - accuracy: 0.9933 - val_loss: 0.5239 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 2s 13ms/step - loss: 0.5172 - accuracy: 1.0000 - val_loss: 0.5017 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4979 - accuracy: 1.0000 - val_loss: 0.4811 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4770 - accuracy: 0.9987 - val_loss: 0.4617 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 2s 12ms/step - loss: 0.4626 - accuracy: 0.9973 - val_loss: 0.4440 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 3s 17ms/step - loss: 0.4401 - accuracy: 1.0000 - val_loss: 0.4271 - val_accuracy: 1.0000\n",
            "62/62 - 1s - loss: 0.4712 - accuracy: 0.9935 - 1s/epoch - 19ms/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resizing (Resizing)         (None, 60, 60, 3)         0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 60, 60, 3)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 10800)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1382528   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,448,834\n",
            "Trainable params: 1,448,834\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}